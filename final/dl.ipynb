{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchmetrics.classification import (\n",
    "    MulticlassAccuracy,\n",
    "    MulticlassPrecision,\n",
    "    MulticlassF1Score,\n",
    "    MulticlassRecall,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "BATCH_SIZE = 128\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "EPOCHS = 100\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path, delimiter=\";\", header=None, names=[\"text\", \"emotion\"])\n",
    "    return df[\"text\"].values, df[\"emotion\"].values\n",
    "\n",
    "\n",
    "# Tokenize and encode labels\n",
    "def tokenize(text, word_to_idx):\n",
    "    return [word_to_idx.get(word, word_to_idx[\"<UNK>\"]) for word in text.split()]\n",
    "\n",
    "\n",
    "def encode_labels(labels):\n",
    "    label_encoder = LabelEncoder()\n",
    "    return label_encoder.fit_transform(labels), label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, train_labels = load_data(\"data/train.txt\")\n",
    "test_texts, test_labels = load_data(\"data/test.txt\")\n",
    "\n",
    "# Build vocabulary\n",
    "word_counter = Counter()\n",
    "for text in train_texts:\n",
    "    word_counter.update(text.split())\n",
    "vocab = [\"<PAD>\", \"<UNK>\"] + [word for word, freq in word_counter.items() if freq > 1]\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "\n",
    "train_labels, label_encoder = encode_labels(train_labels)\n",
    "test_labels, _ = encode_labels(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, word_to_idx, device=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts (list of list of str): List of tokenized sentences.\n",
    "            labels (list of int): List of corresponding labels.\n",
    "            word_to_idx (dict): Dictionary mapping words to indices.\n",
    "            device (torch.device, optional): Device to store the tensors on (e.g., 'cuda' or 'cpu').\n",
    "        \"\"\"\n",
    "        self.device = device if device is not None else torch.device(\"cpu\")\n",
    "\n",
    "        # Preprocess texts and labels into tensors directly\n",
    "        # token_texts = []\n",
    "        # for text in texts:\n",
    "        #     tokens = tokenize(text, word_to_idx)\n",
    "        #     token_texts.append(torch.tensor(tokens, dtype=torch.float32, device=self.device))\n",
    "        # self.texts = token_texts\n",
    "\n",
    "        self.texts = [\n",
    "            torch.tensor(tokenize(text, word_to_idx), device=device) for text in texts\n",
    "        ]\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long, device=self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return preprocessed tensors\n",
    "        return self.texts[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LSTM Model\n",
    "class LSTMEmotionClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
    "        super(LSTMEmotionClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size,\n",
    "            embed_dim,\n",
    "            # , padding_idx=word_to_idx[\"<PAD>\"]\n",
    "        )\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        x = lstm_out[:, -1, :]\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding function\n",
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    texts = pad_sequence(texts, batch_first=True, padding_value=word_to_idx[\"<PAD>\"])\n",
    "    return texts.to(device), torch.tensor(labels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(predictions, labels, num_classes: int):\n",
    "    accuracy_metric = MulticlassAccuracy(num_classes=num_classes, average=\"micro\")\n",
    "    precision_metric = MulticlassPrecision(num_classes=num_classes, average=\"macro\")\n",
    "    recall_metric = MulticlassRecall(num_classes=num_classes, average=\"macro\")\n",
    "    f1_metric= MulticlassF1Score(num_classes=num_classes, average=\"macro\")\n",
    "\n",
    "    predictions = torch.tensor(predictions)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    accuracy = accuracy_metric(predictions, labels)\n",
    "    precision = precision_metric(predictions, labels)\n",
    "    recall = recall_metric(predictions, labels)\n",
    "    f1 = f1_metric(predictions, labels)\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "def evaluate_model(model: nn.Module, data_loader: DataLoader, num_classes: int):\n",
    "    model.eval()\n",
    "\n",
    "    predicted_acum = []\n",
    "    labels_acum = []\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in data_loader:\n",
    "            outputs = model(texts)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            predicted_acum.extend(predicted.tolist())\n",
    "            labels_acum.extend(labels.tolist())\n",
    "\n",
    "    accuracy, precision, recall, f1 = calc_metrics(\n",
    "        predicted_acum, labels_acum, num_classes\n",
    "    )\n",
    "\n",
    "    print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "    print(f\"Precision: {precision*100:.2f}%\")\n",
    "    print(f\"Recall: {recall*100:.2f}%\")\n",
    "    print(f\"F1 score: {f1*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Data Loaders\n",
    "train_dataset = EmotionDataset(train_texts, train_labels, word_to_idx, device)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_dataset = EmotionDataset(test_texts, test_labels, word_to_idx, device)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Model initialization\n",
    "    model = LSTMEmotionClassifier(\n",
    "        vocab_size, EMBEDDING_DIM, HIDDEN_DIM, len(label_encoder.classes_)\n",
    "    )\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        start = time.time()\n",
    "        for texts, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        end = time.time()\n",
    "        epoch_time = end - start\n",
    "        # print(f\"Epoch {epoch+1}, Loss: {loss.item()}, Time: {epoch_time:.2f}s\")\n",
    "    \n",
    "    return model\n",
    "    # Save the model to disk\n",
    "    # torch.save(model.state_dict(), \"lstm_emotion_classifier_model.pth\")\n",
    "    # evaluate_model(model, test_loader, len(label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.5801793336868286, Time: 0.64s\n",
      "Epoch 2, Loss: 1.5696349143981934, Time: 0.59s\n",
      "Epoch 3, Loss: 1.5548969507217407, Time: 0.57s\n",
      "Epoch 4, Loss: 1.5812246799468994, Time: 0.55s\n",
      "Epoch 5, Loss: 1.6175702810287476, Time: 0.55s\n",
      "Epoch 6, Loss: 1.6254268884658813, Time: 0.54s\n",
      "Epoch 7, Loss: 1.550646185874939, Time: 0.53s\n",
      "Epoch 8, Loss: 1.3567177057266235, Time: 0.54s\n",
      "Epoch 9, Loss: 0.9782931208610535, Time: 0.56s\n",
      "Epoch 10, Loss: 0.6881482601165771, Time: 0.53s\n",
      "Epoch 11, Loss: 0.5787520408630371, Time: 0.54s\n",
      "Epoch 12, Loss: 0.45572108030319214, Time: 0.57s\n",
      "Epoch 13, Loss: 0.27451714873313904, Time: 0.68s\n",
      "Epoch 14, Loss: 0.2064484804868698, Time: 0.68s\n",
      "Epoch 15, Loss: 0.21109049022197723, Time: 0.69s\n",
      "Epoch 16, Loss: 0.1710818111896515, Time: 0.58s\n",
      "Epoch 17, Loss: 0.11758346855640411, Time: 0.58s\n",
      "Epoch 18, Loss: 0.07628409564495087, Time: 0.58s\n",
      "Epoch 19, Loss: 0.055165719240903854, Time: 0.61s\n",
      "Epoch 20, Loss: 0.05386904627084732, Time: 0.57s\n",
      "Epoch 21, Loss: 0.06495389342308044, Time: 0.58s\n",
      "Epoch 22, Loss: 0.047582440078258514, Time: 0.55s\n",
      "Epoch 23, Loss: 0.07900944352149963, Time: 0.56s\n",
      "Epoch 24, Loss: 0.02914600446820259, Time: 0.55s\n",
      "Epoch 25, Loss: 0.07949567586183548, Time: 0.55s\n",
      "Epoch 26, Loss: 0.032923005521297455, Time: 0.56s\n",
      "Epoch 27, Loss: 0.03426158428192139, Time: 0.55s\n",
      "Epoch 28, Loss: 0.08371584862470627, Time: 0.58s\n",
      "Epoch 29, Loss: 0.03794749826192856, Time: 0.55s\n",
      "Epoch 30, Loss: 0.007116318214684725, Time: 0.57s\n",
      "Epoch 31, Loss: 0.029591472819447517, Time: 0.56s\n",
      "Epoch 32, Loss: 0.02774076722562313, Time: 0.62s\n",
      "Epoch 33, Loss: 0.024065891280770302, Time: 0.58s\n",
      "Epoch 34, Loss: 0.02781090885400772, Time: 0.55s\n",
      "Epoch 35, Loss: 0.03593892231583595, Time: 0.56s\n",
      "Epoch 36, Loss: 0.012649688869714737, Time: 0.55s\n",
      "Epoch 37, Loss: 0.021319812163710594, Time: 0.57s\n",
      "Epoch 38, Loss: 0.0021266485564410686, Time: 0.55s\n",
      "Epoch 39, Loss: 0.004852563142776489, Time: 0.55s\n",
      "Epoch 40, Loss: 0.0019616223871707916, Time: 0.54s\n",
      "Epoch 41, Loss: 0.003976527135819197, Time: 0.55s\n",
      "Epoch 42, Loss: 0.030152158811688423, Time: 0.55s\n",
      "Epoch 43, Loss: 0.05395972728729248, Time: 0.54s\n",
      "Epoch 44, Loss: 0.027553671970963478, Time: 0.54s\n",
      "Epoch 45, Loss: 0.013850188814103603, Time: 0.54s\n",
      "Epoch 46, Loss: 0.06965356320142746, Time: 0.54s\n",
      "Epoch 47, Loss: 0.007037782575935125, Time: 0.54s\n",
      "Epoch 48, Loss: 0.032273661345243454, Time: 0.55s\n",
      "Epoch 49, Loss: 0.008999542333185673, Time: 0.54s\n",
      "Epoch 50, Loss: 0.0006586426752619445, Time: 0.55s\n",
      "Epoch 51, Loss: 0.011312638409435749, Time: 0.55s\n",
      "Epoch 52, Loss: 0.022892342880368233, Time: 0.54s\n",
      "Epoch 53, Loss: 0.002083619823679328, Time: 0.55s\n",
      "Epoch 54, Loss: 0.0013321413425728679, Time: 0.54s\n",
      "Epoch 55, Loss: 0.0006504229386337101, Time: 0.55s\n",
      "Epoch 56, Loss: 0.0004424563085194677, Time: 0.54s\n",
      "Epoch 57, Loss: 0.00041008988046087325, Time: 0.55s\n",
      "Epoch 58, Loss: 0.03576257452368736, Time: 0.54s\n",
      "Epoch 59, Loss: 0.05865655094385147, Time: 0.55s\n",
      "Epoch 60, Loss: 0.039993613958358765, Time: 0.54s\n",
      "Epoch 61, Loss: 0.003616679459810257, Time: 0.55s\n",
      "Epoch 62, Loss: 0.006118594668805599, Time: 0.54s\n",
      "Epoch 63, Loss: 0.0009736379724927247, Time: 0.54s\n",
      "Epoch 64, Loss: 0.05291210487484932, Time: 0.55s\n",
      "Epoch 65, Loss: 0.006705662235617638, Time: 0.63s\n",
      "Epoch 66, Loss: 0.01616240292787552, Time: 0.65s\n",
      "Epoch 67, Loss: 0.009155289269983768, Time: 0.59s\n",
      "Epoch 68, Loss: 0.01254207082092762, Time: 0.61s\n",
      "Epoch 69, Loss: 0.0019716343376785517, Time: 0.55s\n",
      "Epoch 70, Loss: 0.00019752103253267705, Time: 0.54s\n",
      "Epoch 71, Loss: 0.00042913813376799226, Time: 0.55s\n",
      "Epoch 72, Loss: 0.0010792139219120145, Time: 0.55s\n",
      "Epoch 73, Loss: 0.006106563378125429, Time: 0.64s\n",
      "Epoch 74, Loss: 0.00020495965145528316, Time: 0.63s\n",
      "Epoch 75, Loss: 0.00934202317148447, Time: 0.54s\n",
      "Epoch 76, Loss: 0.00017328861576970667, Time: 0.54s\n",
      "Epoch 77, Loss: 0.00011663370969472453, Time: 0.55s\n",
      "Epoch 78, Loss: 0.009700567461550236, Time: 0.56s\n",
      "Epoch 79, Loss: 0.0063405209220945835, Time: 0.55s\n",
      "Epoch 80, Loss: 0.0028175392653793097, Time: 0.55s\n",
      "Epoch 81, Loss: 0.0071314191445708275, Time: 0.55s\n",
      "Epoch 82, Loss: 7.062628719722852e-05, Time: 0.55s\n",
      "Epoch 83, Loss: 8.226808859035373e-05, Time: 0.55s\n",
      "Epoch 84, Loss: 9.5868315838743e-05, Time: 0.54s\n",
      "Epoch 85, Loss: 5.561010766541585e-05, Time: 0.54s\n",
      "Epoch 86, Loss: 0.013532099314033985, Time: 0.55s\n",
      "Epoch 87, Loss: 0.001202606363222003, Time: 0.54s\n",
      "Epoch 88, Loss: 0.006304191891103983, Time: 0.55s\n",
      "Epoch 89, Loss: 0.0001047155456035398, Time: 0.54s\n",
      "Epoch 90, Loss: 0.008881166577339172, Time: 0.54s\n",
      "Epoch 91, Loss: 0.0184222012758255, Time: 0.54s\n",
      "Epoch 92, Loss: 0.00752647640183568, Time: 0.56s\n",
      "Epoch 93, Loss: 0.012820110656321049, Time: 0.59s\n",
      "Epoch 94, Loss: 0.01314620766788721, Time: 0.56s\n",
      "Epoch 95, Loss: 0.0013477456523105502, Time: 0.54s\n",
      "Epoch 96, Loss: 0.019159814342856407, Time: 0.55s\n",
      "Epoch 97, Loss: 0.00027501373551785946, Time: 0.54s\n",
      "Epoch 98, Loss: 0.0003514816053211689, Time: 0.54s\n",
      "Epoch 99, Loss: 0.0001758796424837783, Time: 0.54s\n",
      "Epoch 100, Loss: 0.014180249534547329, Time: 0.55s\n",
      "Average training time: 56.28s\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "for i in range(1):\n",
    "    start = time.time()\n",
    "    model = train()\n",
    "    end = time.time()\n",
    "    times.append(end-start)\n",
    "\n",
    "torch.save(model.state_dict(), \"lstm_emotion_classifier_model.pth\")\n",
    "print(f\"Average training time: {sum(times)/len(times):.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate():\n",
    "    # Validate the loaded model\n",
    "    loaded_model = LSTMEmotionClassifier(\n",
    "        vocab_size, EMBEDDING_DIM, HIDDEN_DIM, len(label_encoder.classes_)\n",
    "    )\n",
    "    loaded_model.load_state_dict(torch.load(\"lstm_emotion_classifier_model.pth\"))\n",
    "    loaded_model.to(device)\n",
    "    # Load and process validation data\n",
    "    val_texts, val_labels = load_data(os.path.join(os.getcwd(), \"data/val.txt\"))\n",
    "\n",
    "    val_labels = label_encoder.transform(val_labels)\n",
    "    print(val_labels)\n",
    "    val_dataset = EmotionDataset(val_texts, val_labels, word_to_idx, device)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # Evaluate the loaded model on validation data\n",
    "    evaluate_model(loaded_model, val_loader, len(label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 4 3 ... 2 2 2]\n",
      "Accuracy: 90.75%\n",
      "Precision: 87.51%\n",
      "Recall: 86.23%\n",
      "F1 score: 86.79%\n"
     ]
    }
   ],
   "source": [
    "validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    loaded_model = LSTMEmotionClassifier(\n",
    "        vocab_size, EMBEDDING_DIM, HIDDEN_DIM, len(label_encoder.classes_)\n",
    "    )\n",
    "    loaded_model.load_state_dict(torch.load(\"lstm_emotion_classifier_model.pth\"))\n",
    "    loaded_model.to(device)\n",
    "    # Load and process validation data\n",
    "    val_texts, val_labels = load_data(os.path.join(os.getcwd(), \"data/val.txt\"))\n",
    "\n",
    "    val_labels = label_encoder.transform(val_labels)\n",
    "    val_dataset = EmotionDataset(val_texts, val_labels, word_to_idx, device)\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in val_loader:\n",
    "            outputs = loaded_model(texts)\n",
    "            _, predicted_labels = torch.max(outputs.data, 1)\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    return end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation time: 0.04673s\n"
     ]
    }
   ],
   "source": [
    "prediction_times = []\n",
    "for i in range(100):\n",
    "    prediction_times.append(predict())\n",
    "\n",
    "print(f\"Average validation time: {sum(prediction_times)/len(prediction_times):.5f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
