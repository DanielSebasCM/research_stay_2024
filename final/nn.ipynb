{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchmetrics.classification import (\n",
    "    MulticlassAccuracy,\n",
    "    MulticlassPrecision,\n",
    "    MulticlassF1Score,\n",
    "    MulticlassRecall,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MAX_SEQ_LENGTH = 25\n",
    "EPOCHS = 100\n",
    "EMBED_DIM = 100\n",
    "BATCH_SIZE = 128\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize and pad/truncate\n",
    "def tokenize(text, max_length):\n",
    "    tokens = re.findall(r\"\\w+\", text.lower())\n",
    "    if len(tokens) > max_length:\n",
    "        return tokens[:max_length]\n",
    "    return tokens + [\"<PAD>\"] * (max_length - len(tokens))\n",
    "\n",
    "\n",
    "def load_data(file_path, max_length):\n",
    "    texts, labels = [], []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            text, label = line.strip().split(\";\")\n",
    "            texts.append(tokenize(text, max_length))\n",
    "            labels.append(label)\n",
    "    \n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, word_to_idx, device=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts (list of list of str): List of tokenized sentences.\n",
    "            labels (list of int): List of corresponding labels.\n",
    "            word_to_idx (dict): Dictionary mapping words to indices.\n",
    "            device (torch.device, optional): Device to store the tensors on (e.g., 'cuda' or 'cpu').\n",
    "        \"\"\"\n",
    "        self.device = device if device is not None else torch.device(\"cpu\")\n",
    "\n",
    "        # Preprocess texts and labels into tensors directly\n",
    "        self.texts = torch.tensor(\n",
    "            [\n",
    "                [word_to_idx.get(word, word_to_idx[\"<UNK>\"]) for word in text]\n",
    "                for text in texts\n",
    "            ],\n",
    "            dtype=torch.long,\n",
    "            device=self.device,\n",
    "        )\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long, device=self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return preprocessed tensors\n",
    "        return self.texts[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, EMBED_DIM, num_classes):\n",
    "        super(EmotionClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, EMBED_DIM)\n",
    "        self.fc = nn.Linear(EMBED_DIM, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(predictions, labels, num_classes: int):\n",
    "    accuracy_metric = MulticlassAccuracy(num_classes=num_classes, average=\"micro\")\n",
    "    precision_metric = MulticlassPrecision(num_classes=num_classes, average=\"macro\")\n",
    "    recall_metric = MulticlassRecall(num_classes=num_classes, average=\"macro\")\n",
    "    f1_metric= MulticlassF1Score(num_classes=num_classes, average=\"macro\")\n",
    "\n",
    "    predictions = torch.tensor(predictions)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    accuracy = accuracy_metric(predictions, labels)\n",
    "    precision = precision_metric(predictions, labels)\n",
    "    recall = recall_metric(predictions, labels)\n",
    "    f1 = f1_metric(predictions, labels)\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "def evaluate_model(model: nn.Module, data_loader: DataLoader, num_classes: int):\n",
    "    model.eval()\n",
    "\n",
    "    predicted_acum = []\n",
    "    labels_acum = []\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in data_loader:\n",
    "            outputs = model(texts)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            predicted_acum.extend(predicted.tolist())\n",
    "            labels_acum.extend(labels.tolist())\n",
    "    \n",
    "    accuracy, precision, recall, f1 = calc_metrics(predicted_acum, labels_acum, num_classes)\n",
    "            \n",
    "    print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "    print(f\"Precision: {precision*100:.2f}%\")\n",
    "    print(f\"Recall: {recall*100:.2f}%\")\n",
    "    print(f\"F1 score: {f1*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process data\n",
    "train_texts, train_labels = load_data(\n",
    "    os.path.join(os.getcwd(), \"data/train.txt\"), MAX_SEQ_LENGTH\n",
    ")\n",
    "test_texts, test_labels = load_data(\n",
    "    os.path.join(os.getcwd(), \"data/test.txt\"), MAX_SEQ_LENGTH\n",
    ")\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_labels)\n",
    "test_labels = label_encoder.transform(test_labels)\n",
    "\n",
    "# Build vocabulary\n",
    "all_words = [word for text in train_texts for word in text]\n",
    "word_counts = Counter(all_words)\n",
    "non_unique_words = [word for word, count in word_counts.items() if count > 1]\n",
    "vocab = [\"<PAD>\", \"<UNK>\"] + non_unique_words\n",
    "\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "# Model parameters\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "num_classes = len(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Model, Loss, and Optimizer\n",
    "    model = EmotionClassifier(vocab_size, EMBED_DIM, num_classes)\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Data loaders\n",
    "    train_dataset = EmotionDataset(train_texts, train_labels, word_to_idx, device)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    test_dataset = EmotionDataset(test_texts, test_labels, word_to_idx, device)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        start = time.time()\n",
    "        for texts, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        end = time.time()\n",
    "        epoch_time = end - start\n",
    "        # print(f\"Epoch {epoch+1}, Loss: {loss.item()}, Time: {epoch_time:.2f}s\")\n",
    "        \n",
    "    return model\n",
    "    # Save the model to disk\n",
    "    # torch.save(model.state_dict(), \"emotion_classifier_model.pth\")\n",
    "\n",
    "    # evaluate_model(model, test_loader, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 5 times...\n",
      "Model trained 5 times\n",
      "[14.280154705047607, 14.150193691253662, 14.089772939682007, 14.065330743789673, 13.933905124664307]\n",
      "Average training time: 14.10s\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "\n",
    "print(\"Training model 5 times...\")\n",
    "for i in range(5):\n",
    "    start = time.time()\n",
    "    model = train()\n",
    "    end = time.time()\n",
    "    times.append(end - start)\n",
    "\n",
    "torch.save(model.state_dict(), \"emotion_classifier_model.pth\")\n",
    "\n",
    "print(\"Model trained 5 times\")\n",
    "print(times)\n",
    "print(f\"Average training time: {sum(times)/len(times):.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate():\n",
    "    loaded_model = EmotionClassifier(vocab_size, EMBED_DIM, num_classes)\n",
    "    loaded_model.load_state_dict(torch.load(\"emotion_classifier_model.pth\"))\n",
    "    loaded_model.to(device)\n",
    "    # Load and process validation data\n",
    "    val_texts, val_labels = load_data(\n",
    "        os.path.join(os.getcwd(), \"data/val.txt\"), MAX_SEQ_LENGTH\n",
    "    )\n",
    "\n",
    "    val_labels = label_encoder.transform(val_labels)\n",
    "    val_dataset = EmotionDataset(val_texts, val_labels, word_to_idx, device)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    print(\"Validation results:\")\n",
    "    evaluate_model(loaded_model, val_loader, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results:\n",
      "Accuracy: 84.90%\n",
      "Precision: 81.99%\n",
      "Recall: 80.85%\n",
      "F1 score: 81.38%\n"
     ]
    }
   ],
   "source": [
    "validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    loaded_model = EmotionClassifier(vocab_size, EMBED_DIM, num_classes)\n",
    "    loaded_model.load_state_dict(torch.load(\"emotion_classifier_model.pth\"))\n",
    "    loaded_model.to(device)\n",
    "    # Load and process validation data\n",
    "    val_texts, val_labels = load_data(\n",
    "        os.path.join(os.getcwd(), \"data/val.txt\"), MAX_SEQ_LENGTH\n",
    "    )\n",
    "\n",
    "    val_labels = label_encoder.transform(val_labels)\n",
    "    val_dataset = EmotionDataset(val_texts, val_labels, word_to_idx, device)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in val_loader:\n",
    "            outputs = loaded_model(texts)\n",
    "            _, predicted_labels = torch.max(outputs.data, 1)\n",
    "    \n",
    "    end = time.time()\n",
    "\n",
    "    return end-start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferring 100 times, 2000 inferences each...\n",
      "Average inference time: 0.00936s\n"
     ]
    }
   ],
   "source": [
    "prediction_times = []\n",
    "print(\"Inferring 100 times, 2000 inferences each...\")\n",
    "for i in range(100):\n",
    "    prediction_times.append(predict())\n",
    "\n",
    "print(f\"Average inference time: {sum(prediction_times)/len(prediction_times):.5f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
