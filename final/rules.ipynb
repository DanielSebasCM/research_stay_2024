{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cajas/Archivos/tec/semestre_7/research_stay_2024/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from torchmetrics.classification import (\n",
    "    MulticlassAccuracy,\n",
    "    MulticlassPrecision,\n",
    "    MulticlassF1Score,\n",
    "    MulticlassRecall,\n",
    ")\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = [\"love\", \"fear\", \"sadness\", \"surprise\", \"joy\", \"anger\"]\n",
    "\n",
    "stop_words = set(\n",
    "    [\n",
    "        \"i\",\n",
    "        \"me\",\n",
    "        \"my\",\n",
    "        \"myself\",\n",
    "        \"we\",\n",
    "        \"our\",\n",
    "        \"ours\",\n",
    "        \"ourselves\",\n",
    "        \"you\",\n",
    "        \"you're\",\n",
    "        \"you've\",\n",
    "        \"you'll\",\n",
    "        \"you'd\",\n",
    "        \"your\",\n",
    "        \"yours\",\n",
    "        \"yourself\",\n",
    "        \"yourselves\",\n",
    "        \"he\",\n",
    "        \"him\",\n",
    "        \"his\",\n",
    "        \"himself\",\n",
    "        \"she\",\n",
    "        \"she's\",\n",
    "        \"her\",\n",
    "        \"hers\",\n",
    "        \"herself\",\n",
    "        \"it\",\n",
    "        \"it's\",\n",
    "        \"its\",\n",
    "        \"itself\",\n",
    "        \"they\",\n",
    "        \"them\",\n",
    "        \"their\",\n",
    "        \"theirs\",\n",
    "        \"themselves\",\n",
    "        \"what\",\n",
    "        \"which\",\n",
    "        \"who\",\n",
    "        \"whom\",\n",
    "        \"this\",\n",
    "        \"that\",\n",
    "        \"that'll\",\n",
    "        \"these\",\n",
    "        \"those\",\n",
    "        \"am\",\n",
    "        \"is\",\n",
    "        \"are\",\n",
    "        \"was\",\n",
    "        \"were\",\n",
    "        \"be\",\n",
    "        \"been\",\n",
    "        \"being\",\n",
    "        \"have\",\n",
    "        \"has\",\n",
    "        \"had\",\n",
    "        \"having\",\n",
    "        \"do\",\n",
    "        \"does\",\n",
    "        \"did\",\n",
    "        \"doing\",\n",
    "        \"a\",\n",
    "        \"an\",\n",
    "        \"the\",\n",
    "        \"and\",\n",
    "        \"but\",\n",
    "        \"if\",\n",
    "        \"or\",\n",
    "        \"because\",\n",
    "        \"as\",\n",
    "        \"until\",\n",
    "        \"while\",\n",
    "        \"of\",\n",
    "        \"at\",\n",
    "        \"by\",\n",
    "        \"for\",\n",
    "        \"with\",\n",
    "        \"about\",\n",
    "        \"against\",\n",
    "        \"between\",\n",
    "        \"into\",\n",
    "        \"through\",\n",
    "        \"during\",\n",
    "        \"before\",\n",
    "        \"after\",\n",
    "        \"above\",\n",
    "        \"below\",\n",
    "        \"to\",\n",
    "        \"from\",\n",
    "        \"up\",\n",
    "        \"down\",\n",
    "        \"in\",\n",
    "        \"out\",\n",
    "        \"on\",\n",
    "        \"off\",\n",
    "        \"over\",\n",
    "        \"under\",\n",
    "        \"again\",\n",
    "        \"further\",\n",
    "        \"then\",\n",
    "        \"once\",\n",
    "        \"here\",\n",
    "        \"there\",\n",
    "        \"when\",\n",
    "        \"where\",\n",
    "        \"why\",\n",
    "        \"how\",\n",
    "        \"all\",\n",
    "        \"any\",\n",
    "        \"both\",\n",
    "        \"each\",\n",
    "        \"few\",\n",
    "        \"more\",\n",
    "        \"most\",\n",
    "        \"other\",\n",
    "        \"some\",\n",
    "        \"such\",\n",
    "        \"no\",\n",
    "        \"nor\",\n",
    "        \"not\",\n",
    "        \"only\",\n",
    "        \"own\",\n",
    "        \"same\",\n",
    "        \"so\",\n",
    "        \"than\",\n",
    "        \"too\",\n",
    "        \"very\",\n",
    "        \"s\",\n",
    "        \"t\",\n",
    "        \"can\",\n",
    "        \"will\",\n",
    "        \"just\",\n",
    "        \"don\",\n",
    "        \"don't\",\n",
    "        \"should\",\n",
    "        \"should've\",\n",
    "        \"now\",\n",
    "        \"d\",\n",
    "        \"ll\",\n",
    "        \"m\",\n",
    "        \"o\",\n",
    "        \"re\",\n",
    "        \"ve\",\n",
    "        \"y\",\n",
    "        \"ain\",\n",
    "        \"aren\",\n",
    "        \"aren't\",\n",
    "        \"couldn\",\n",
    "        \"couldn't\",\n",
    "        \"didn\",\n",
    "        \"didn't\",\n",
    "        \"doesn\",\n",
    "        \"doesn't\",\n",
    "        \"hadn\",\n",
    "        \"hadn't\",\n",
    "        \"hasn\",\n",
    "        \"hasn't\",\n",
    "        \"haven\",\n",
    "        \"haven't\",\n",
    "        \"isn\",\n",
    "        \"isn't\",\n",
    "        \"ma\",\n",
    "        \"mightn\",\n",
    "        \"mightn't\",\n",
    "        \"mustn\",\n",
    "        \"mustn't\",\n",
    "        \"needn\",\n",
    "        \"needn't\",\n",
    "        \"shan\",\n",
    "        \"shan't\",\n",
    "        \"shouldn\",\n",
    "        \"shouldn't\",\n",
    "        \"wasn\",\n",
    "        \"wasn't\",\n",
    "        \"weren\",\n",
    "        \"weren't\",\n",
    "        \"won\",\n",
    "        \"won't\",\n",
    "        \"wouldn\",\n",
    "        \"wouldn't\",\n",
    "        \"feeling\",\n",
    "        \"feel\",\n",
    "        \"really\",\n",
    "        \"im\",\n",
    "        \"like\",\n",
    "        \"know\",\n",
    "        \"get\",\n",
    "        \"ive\",\n",
    "        \"im'\",\n",
    "        \"stil\",\n",
    "        \"even\",\n",
    "        \"time\",\n",
    "        \"want\",\n",
    "        \"one\",\n",
    "        \"cant\",\n",
    "        \"think\",\n",
    "        \"go\",\n",
    "        \"much\",\n",
    "        \"never\",\n",
    "        \"day\",\n",
    "        \"back\",\n",
    "        \"see\",\n",
    "        \"still\",\n",
    "        \"make\",\n",
    "        \"thing\",\n",
    "        \"would\",\n",
    "        \"would'\",\n",
    "        \"could'\",\n",
    "        \"little\",\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_lexicon(file_path, counter_most_common):\n",
    "\n",
    "    emotion_counters = {emotion: Counter() for emotion in emotions}\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            text, emotion = line.strip().split(\";\")\n",
    "            if emotion in emotions:\n",
    "                words = [\n",
    "                    word\n",
    "                    for word in re.findall(r\"\\w+\", text.lower())\n",
    "                    if word not in stop_words\n",
    "                ]\n",
    "                emotion_counters[emotion].update(words)\n",
    "\n",
    "    emotion_lexicon = {\n",
    "        emotion: [word for word, _ in counter.most_common(counter_most_common)]\n",
    "        for emotion, counter in emotion_counters.items()\n",
    "    }\n",
    "\n",
    "    return emotion_lexicon\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(file_path, lexicon):\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    lexicon = {\n",
    "        emotion: set(map(str.lower, words)) for emotion, words in lexicon.items()\n",
    "    }\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            text, actual_emotion = line.strip().split(\";\")\n",
    "            words = set(re.findall(r\"\\w+\", text.lower()))\n",
    "\n",
    "            emotion_scores = defaultdict(int)\n",
    "\n",
    "            for emotion, emotion_words in lexicon.items():\n",
    "                common_words = words & emotion_words\n",
    "                emotion_scores[emotion] += len(common_words)\n",
    "\n",
    "            predicted_emotion = max(emotion_scores, key=emotion_scores.get)\n",
    "\n",
    "            predictions.append(predicted_emotion)\n",
    "            labels.append(actual_emotion)\n",
    "\n",
    "    return predictions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_results(predictions, labels, num_classes: int):\n",
    "    accuracy_metric = MulticlassAccuracy(num_classes=num_classes, average=\"micro\")\n",
    "    precision_metric = MulticlassPrecision(num_classes=num_classes, average=\"macro\")\n",
    "    recall_metric = MulticlassRecall(num_classes=num_classes, average=\"macro\")\n",
    "    f1_metric = MulticlassF1Score(num_classes=num_classes, average=\"macro\")\n",
    "\n",
    "    predictions = [emotions.index(prediction) for prediction in predictions]\n",
    "    labels = [emotions.index(label) for label in labels]\n",
    "\n",
    "    predictions = torch.tensor(predictions)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    accuracy = accuracy_metric(predictions, labels)\n",
    "    precision = precision_metric(predictions, labels)\n",
    "    recall = recall_metric(predictions, labels)\n",
    "    f1 = f1_metric(predictions, labels)\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    train_file_path = \"data/train.txt\"\n",
    "    test_file_path = \"data/test.txt\"\n",
    "\n",
    "    # Hyperparameters:\n",
    "    # increment_step = 5\n",
    "    # max_word_count = 2000\n",
    "\n",
    "    # for counter_most_common in range(5, max_word_count, increment_step):\n",
    "    counter_most_common = 350\n",
    "    start = time.time()\n",
    "    lexicon = create_lexicon(train_file_path, counter_most_common)\n",
    "    train_time = time.time() - start\n",
    "    start = time.time()\n",
    "    predictions, labels = predict(test_file_path, lexicon)\n",
    "    inference_time = time.time() - start\n",
    "\n",
    "    accuracy, precision, recall, f1 = evaluate_results(\n",
    "        predictions, labels, num_classes=len(emotions)\n",
    "    )\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1: {f1:.2f}\")\n",
    "\n",
    "    print(f\"Train Time: {train_time:.2f} seconds\")\n",
    "    print(f\"Inference Time: {inference_time:.2f} seconds\")\n",
    "\n",
    "    print(\"End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'final/data/train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m, in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m counter_most_common \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m350\u001b[39m\n\u001b[1;32m     11\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 12\u001b[0m lexicon \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_lexicon\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcounter_most_common\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m train_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m     14\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m, in \u001b[0;36mcreate_lexicon\u001b[0;34m(file_path, counter_most_common)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_lexicon\u001b[39m(file_path, counter_most_common):\n\u001b[1;32m      3\u001b[0m     emotion_counters \u001b[38;5;241m=\u001b[39m {emotion: Counter() \u001b[38;5;28;01mfor\u001b[39;00m emotion \u001b[38;5;129;01min\u001b[39;00m emotions}\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file:\n\u001b[1;32m      7\u001b[0m             text, emotion \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Archivos/tec/semestre_7/research_stay_2024/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'final/data/train.txt'"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
